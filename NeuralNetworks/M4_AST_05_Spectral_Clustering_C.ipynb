{"cells":[{"cell_type":"markdown","metadata":{"id":"g1IiUcV1c4KG"},"source":["# Advanced Certification Program in Computational Data Science\n","## A program by IISc and TalentSprint\n","### Assignment 5: Spectral Clustering"]},{"cell_type":"markdown","metadata":{"id":"p_otMk5RdD1w"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"pEkFldW1dFKS"},"source":["At the end of the experiment, you will be able to\n","\n","* understand what is graph clustering\n","* implement Spectral Clustering and KMeans Clustering algorithm"]},{"cell_type":"markdown","metadata":{"id":"FHS1gB91gBXm"},"source":["### Spectral Clustering"]},{"cell_type":"markdown","metadata":{"id":"S54b7gb0gBXn"},"source":["Spectral clustering is a popular unsupervised machine learning algorithm that often outperforms other approaches. In addition, spectral clustering is very simple to implement and can be solved efficiently by standard linear algebra methods. In spectral clustering, the affinity, and not the absolute location (i.e. k-means), determines what points fall under which cluster. The latter is particularly useful in tackling problems where the data forms complicated shapes.\n","\n","**Algorithm**\n","\n","The algorithm can be broken down into 4 basic steps.\n","1. Construct a similarity graph\n","2. Determine the Adjacency matrix W, Degree matrix D and the Laplacian matrix L\n","3. Compute the eigenvectors of the matrix L\n","4. Using the second smallest eigenvector as input, train a k-means model and use it to classify the data\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WB5kZiwqgBXn"},"source":["The Scikit-learn API provides SpectralClustering class to implement spectral clustering method in Python. The SpectralClustering applies the clustering to a projection of the normalized Laplacian.\n","\n","In this example, we'll briefly learn how to cluster and visualize data with SpectralClustering in Python. The example covers:\n","\n","1. Preparing the data\n","2. Clustering with SpectralClustering and KMeans, for making comparison between the two algorithms.\n","3. Plot the results"]},{"cell_type":"markdown","metadata":{"id":"BNLA8HiKxQhc"},"source":["### Setup Steps:"]},{"cell_type":"markdown","metadata":{"id":"thfPJ9cpwnh2"},"source":["### Import required packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"spr902IfqlBD"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","from sklearn.cluster import KMeans, SpectralClustering\n","import sklearn.datasets\n","from sklearn.preprocessing import StandardScaler\n","# Scprep provides an all-in-one framework for loading, preprocessing, and plotting matrices in Python\n","import scprep              \n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{"id":"M-TWF7HFgBXn"},"source":["#### Preparing the data\n","\n","Here, we are going to implement the SpectralClustering algorithm as well as the KMeans algorithm.\n","\n","We'll be working with 2 datasets all in  $R^2$: \n","\n","1. Circles - two circles, one circumscribed by the other\n","2. Moons - Two interleaving half circles\n","\n","Because we're generating these from scratch, we get to change some parameters of their distributions. Generally, we can change:\n","\n","1. noise - the amount of Gaussian noise added to each point\n","2. n_samples - the number of points generated\n","3. factor / cluster_std - some parameters affecting shape\n"," "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sIQQGGDKgBXn"},"outputs":[],"source":["np.random.seed(0)\n","\n","''' Generate datasets. We choose the size big enough to see the scalability\n","    of the algorithms, but not too big to avoid too long running times '''\n","  \n","n_samples = 1500\n","\n","# Circles dataset\n","noisy_circles = sklearn.datasets.make_circles(\n","    n_samples=n_samples, \n","    # Scale factor between inner and outer circle\n","    factor=.5,\n","    # Gaussian noise added to each point\n","    noise=.05)\n","\n","# Moons dataset\n","noisy_moons = sklearn.datasets.make_moons(n_samples=n_samples, \n","                                          noise=.05)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q9Fh8Kd4KUsw"},"outputs":[],"source":["# Associate each dataset with the correct number of clusters\n","default_base = {'n_clusters': 3}\n","\n","generated_datasets = [\n","    (noisy_circles, {'n_clusters': 2}),\n","    (noisy_moons, {'n_clusters': 2})\n","]"]},{"cell_type":"markdown","metadata":{"id":"8jYVzycGFJaM"},"source":["#### Plot ground truth cluster assignments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DsefhyOdFJaP"},"outputs":[],"source":["fig, axes = plt.subplots(1,2,figsize=(6,3))\n","\n","for i, (dataset, _) in enumerate(generated_datasets):\n","    ax = axes[i]\n","    X, y = dataset\n","    \n","    # Normalize dataset for easier parameter selection\n","    X = StandardScaler().fit_transform(X)\n","    scprep.plot.scatter2d(X, c=y, \n","                          ticks=None, ax=ax, \n","                          xlabel='x0', ylabel='x1',\n","                         legend=False)\n","    \n","fig.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"er_fVGAhFJaT"},"source":["### Run clustering algorithms and plot results\n","\n"]},{"cell_type":"markdown","metadata":{"id":"03ChNZwhgBXo"},"source":["It is an easy to understand data so we will cluster it with spectral cluster method.\n","\n","We will define model by using SpectralClustering class and KMeans class and then we will fit it on x data. The SpectralClustering requires the number of clusters so we'll set `n_cluster`(define above) to n_cluster parameter. We can check the parameters of the class and change them according to your analysis and target data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4C5Mvrg4FJaW"},"outputs":[],"source":["fig, axes = plt.subplots(2,2, figsize=(12, 7.5))\n","plot_title = True\n","\n","for i_dataset, (dataset, cluster_params) in enumerate(generated_datasets):\n","    # update cluster parameters with dataset-specific values\n","    params = default_base.copy()\n","    params.update(cluster_params)\n","\n","    X, y = dataset\n","    \n","    # normalize dataset for easier parameter selection\n","    X = StandardScaler().fit_transform(X)\n","  \n","    # Run clustering algorithms\n","    clusters = []\n","    titles = []\n","    times = []\n","    # KMeans\n","    tic = time.time()\n","    kmeans = KMeans(n_clusters=params['n_clusters'])\n","    clusters.append(kmeans.fit_predict(X))\n","    titles.append('KMeans')\n","    times.append(time.time() - tic)\n","    \n","    # Spectral Clustering\n","    tic = time.time()\n","    spectral = SpectralClustering(\n","        n_clusters=params['n_clusters'], eigen_solver='arpack',\n","        affinity=\"nearest_neighbors\")\n","    clusters.append(spectral.fit_predict(X))\n","    titles.append('Spectral')\n","    times.append(time.time() - tic)\n","\n","    # Plot clustering results for dataset\n","    row_axes = axes[i_dataset]\n","    \n","    for i, ax in enumerate(row_axes.flatten()):\n","        curr_cluster = clusters[i]\n","        if plot_title:\n","            curr_title = '{}'.format(titles[i])\n","        else:\n","            curr_title = None\n","            \n","        scprep.plot.scatter2d(X, c=curr_cluster, title=curr_title, ax=ax,\n","                             legend=False, discrete=True)\n","\n","        # Plot time to run algorithm\n","        plt.text(.99, .01, ('%.2fs' % (times[i])).lstrip('0'),\n","                 transform=ax.transAxes, size=15,\n","                 horizontalalignment='right')\n","    plot_title=False\n","fig.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"B8VaLXd5FJah"},"source":["#### Discussion\n","In groups, change the following features for one or more of the distributions\n","1. `noise` - the amount of Gaussian noise added to each point\n","2. `n_samples` - the number of points generated\n","3. `factor` / `cluster_std` / `transformation` - some parameters affecting shape\n","\n","Try to identify:\n","1. A set of parameters that makes `SpectralClustering` fail on the circles dataset\n","    - `factor=0.5, noise=0.1`\n","\n","2. `KMeans` fail on the three regular blobs.\n","    - `cluster_std=4`"]},{"cell_type":"markdown","metadata":{"id":"MN2WLUOqgBXp"},"source":["In this example, we have briefly learned how to cluster and visualize the data by using the `SpectralClustering` and `KMeans` algorithm in Python."]},{"cell_type":"markdown","metadata":{"id":"iY7ro_8WgBXp"},"source":["<font color='blue'>**Discussion 1:** Compare the Spectral Clustering and KMeans Clustering algorithms and discuss. Draw your conclusions."]},{"cell_type":"markdown","metadata":{"id":"PkG6WHPTgBXp"},"source":["Spectral clustering has many applications in real-life. Here are some of the few use cases of spectral clustering:\n","\n","- **Spam filter:** You know the junk folder in your email inbox? It is the place where emails that have been identified as spam by the algorithm.\n","- **Marketing and Sales:** Personalization and targeting in marketing is big business. \n","- **Classifying network traffic:** Imagine you want to understand the different types of traffic coming to your website. You are particularly interested in understanding which traffic is spam or coming from bots.\n","- **Document analysis:** There are many different reasons why you would want to run an analysis on a document. In this scenario, you want to be able to organize the documents quickly and efficiently."]},{"cell_type":"markdown","metadata":{"id":"bJlVGuwPgBXp"},"source":["### Theory Questions"]},{"cell_type":"markdown","metadata":{"id":"Egdd9c3AgBXp"},"source":["1. What is an Adjacency Matrix in data science?\n","\n"," In mathematics and computer science, an adjacency matrix is a means of representing which vertices of a graph are adjacent to which other vertices. Another matrix representation for a graph is the incidence matrix. Specifically, the adjacency matrix of a finite graph $G$ on $n$ vertices is the $n × n$ matrix where the non-diagonal entry $a_{ij}$ is the number of edges from vertex $i$ to vertex $j$, and the diagonal entry $a_{ii}$, depending on the convention, is either once or twice the number of edges (loops) from vertex $i$ to itself.\n","\n","2. What are the 2 broad approaches for clustering?\n","\n"," * **Compactness** — Points that lie close to each other fall in the same cluster and are compact around the cluster center. The closeness can be measured by the distance between the observations. E.g.: K-Means Clustering\n","\n"," * **Connectivity** — Points that are connected or immediately next to each other are put in the same cluster. Even if the distance between 2 points is less, if they are not connected, they are not clustered together. Spectral clustering is a technique that follows this approach.\n","\n","3. How does Spectral Clustering work?\n"," \n"," In spectral clustering, the data points are treated as nodes of a graph. Thus, clustering is treated as a graph partitioning problem. The nodes are then mapped to a low-dimensional space that can be easily segregated to form clusters. An important point to note is that no assumption is made about the shape/form of the clusters."]}],"metadata":{"colab":{"collapsed_sections":[],"name":"M4_AST_05_Spectral_Clustering_C.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}
