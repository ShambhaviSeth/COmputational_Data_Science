{"cells":[{"cell_type":"markdown","metadata":{"id":"jUxscojnnv0r"},"source":["# Advanced Certification Program in Computational Data Science\n","## A program by IISc and TalentSprint\n","### Assignment 7 Kafka Producer: Apache Kafka Streaming"]},{"cell_type":"markdown","metadata":{"id":"ImAbiiU2nv0u"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"dtaHO9W4nv0v"},"source":["At the end of the experiment, you will be able to\n","\n","* understand what is Kafka and its components\n","* perform real-time data analytics with Kafka"]},{"cell_type":"markdown","metadata":{"id":"iCvtDBFenv0v"},"source":["## Information"]},{"cell_type":"markdown","metadata":{"id":"9Fe8re5Pnv0w"},"source":["### Introduction"]},{"cell_type":"markdown","metadata":{"id":"584kfh_hUVRy"},"source":["Stream processing refers to the processing of data in motion, or in other words, computing on data directly as it is produced or received. The majority of data are born as continuous streams: sensor events, user activity on a website, financial trades, and so on – all these data are created as a series of events over time.\n","\n","Before stream processing, this data was often stored in a database, a file system, or other forms of mass storage. Applications would query the data or compute over the data as needed. Stream Processing turns this paradigm around: the application logic, analytics, and queries exist continuously, and data flows through them continuously.\n","\n","Some of the stream processing frameworks are Apache Flink, Apache Storm, Apache Kafka, and Spark streaming as shown in the figure below.  \n","\n","<img src= \"https://cdn.iisc.talentsprint.com/CDS/Images/Modern_Stream_Processing_Frameworks.jpg\" width= 550 px/>\n","\n","Here we will consider Apache Kafka for streaming.\n","\n","Apache Kafka is an open-source software platform developed by the Apache Software Foundation written in Scala and Java. The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds.\n","\n","Kafka was originally developed by LinkedIn and was subsequently open-sourced in early 2011. \n","\n","Kafka is a distributed publish-subscribe messaging system that is designed to be fast, scalable, and durable. Kafka maintains messages in topics. Producers write data to topics and consumers read from topics. Kafka is a distributed system, topics are partitioned and replicated across multiple nodes.\n","\n","It is based on the commit log, and it allows users to subscribe to it and publish data to any number of systems or real-time applications. Example applications include managing passenger and driver matching at Uber, providing real-time analytics and predictive maintenance for British Gas smart home, and performing numerous real-time services across all of LinkedIn.\n","\n","To know more about the use cases of Kafka click [here](https://kafka.apache.org/documentation.html#uses)."]},{"cell_type":"markdown","metadata":{"id":"oxC7feShUVRz"},"source":["### Components of Kafka cluster"]},{"cell_type":"markdown","metadata":{"id":"-iuRTkSmUVRz"},"source":["The components of Kafka are as follows:\n","\n","* **Log:**\n","Write-ahead log, commit log, transaction log; Each partition in Apache Kafka is a log - a time-ordered, append-only sequence of data, from where data is removed only when a given retention period has been exceeded. Records are appended to the end of the log and can be read in order. The log can also be rewound and records can be skipped\n","over for consumers to read from any point in the partition.\n","\n","* **Record or Message:**\n","Data sent to and from the broker is called a record, a key-value pair. The record contains the topic name and partition number. The Kafka broker keeps records inside topic partitions.\n","\n","* **Broker:**\n","The brokers in a Kafka cluster handle the process of receiving, storing and forwarding the records to the interested consumers. \n","\n","* **Topics:**\n","Records are grouped into categories called topics. A Topic is a category/feed name to which records are stored and published. Example: LogMessage or StockMessage. If we wish to send a record we send it to a specific topic and if we want to read a record we read it from a specific topic.\n","\n","* **Retention period:**\n","Records published to the cluster will stay in the cluster until a configurable retention period has passed. Kafka retains all records for a set amount of time or until a configurable size is reached. The consumption time is not impacted by the size of the log.\n","\n","* **Producer, Producer API:**\n","The processes that publish records/messages into a topic are called producers and are using the producer API.\n","\n","* **Consumer, Consumer API:**\n","The processes that consume records/messages from a topic are called consumers and are using the consumer API.\n","\n","* **Partition:**\n","Topics are divided into one or more partitions, which can be replicated between nodes. Partitions are the unit of parallelism in Kafka. Partitions allow records in a topic to be distributed to multiple brokers. A topic can have any number of partitions that we can specify.\n","\n","<center>\n","<figure>\n","<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/Kafka_offset.png\" width= 450 px/>\n","</figure>\n","</center>\n","\n","\n","* **Offset:**\n","Kafka topics are divided into several partitions, which contain records in an unchangeable sequence. Each record in a partition is assigned and identified by its unique offset.\n","\n","* **Consumer group:**\n","A consumer group includes the set of consumers that are subscribing to a specific topic. Kafka consumers are usually a part of a consumer group. Each consumer in the group is assigned a set of partitions, from which they are able to consume messages. Each consumer in the group\n","will receive records from different subsets of the partitions in the topic.\n","\n","* **ZooKeeper:**\n","Zookeeper is a stand-alone, centralized service, acting across nodes to relieve Kafka from administrative duties. Zookeeper is responsible for controller elections, the configuration of topics, handling access control lists and cluster memberships.\n","\n","* **Instance (“As in a CloudKarafka instance”):**\n","When a CloudKarafka plan is created, we get CloudKarafka instance or an instance of Apache Kafka. It could be a dedicated instance, an Apache Kafka broker, or a shared instance, which gives you five dedicated topics on a shared plan.\n"]},{"cell_type":"markdown","metadata":{"id":"OihZ9gFT4WzP"},"source":["### Kafka Architecture"]},{"cell_type":"markdown","metadata":{"id":"Jy-bMK-2vgdC"},"source":["\n","<img src= \"https://cdn.iisc.talentsprint.com/CDS/Images/KafkaCluster.png\" width= 500 px/>\n","\n","Kafka stores messages that come from arbitrarily many processes called **producers**. The data can be partitioned into different **partitions** within different **topics**. Within a partition, messages are strictly ordered by their **offsets** (the position of a message within a partition) and indexed and stored together with a timestamp. Other processes called **consumers** can read messages from partitions. \n","\n","Kafka runs on a cluster of one or more servers (called **brokers**), and the partitions of all topics are distributed across the cluster nodes. Additionally, partitions are replicated to multiple brokers. This architecture allows Kafka to deliver massive streams of messages in a fault-tolerant fashion."]},{"cell_type":"markdown","metadata":{"id":"fMyKHLWn85L6"},"source":["**Running Producer and Consumer from two separate notebooks**\n","\n","The use of two Colab notebooks is necessary as both producer and consumer files need to be run **simultaneously** and we cannot run two code cells together within the same colab notebook. So while the consumer file is running in another notebook we can run the producer (given in this notebook) file and send messages. The messages sent by the producer will be available at the consumer side. Then we can perform operations on these messages for example print the message, count the number of words in it, compute the rolling mean if numerical data provided, or trigger something if a particular message is received.\n","\n","The steps for the same are given below:\n","\n","* Go to Kafka Consumer notebook and run `consumer.py` file first\n","* While consumer file is running, run `producer.py` file from Kafka Producer notebook\n","* When both files are running, type your message in Producer file and send\n","* The message will be received at the consumer side and output of the operations will be displayed\n","* Stop the Producer cell first and then the corresponding Consumer cell"]},{"cell_type":"markdown","metadata":{"id":"BNLA8HiKxQhc"},"source":["### Setup Steps:"]},{"cell_type":"markdown","metadata":{"id":"1MseB3YVQ7lV"},"source":["### Install Confluent kafka"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6580,"status":"ok","timestamp":1644060468666,"user":{"displayName":"shambhavi seth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh20oin3w2PRiKUhrvgT0XoIOjlTZOhPgvWiKgfFNg=s64","userId":"04318899503633324567"},"user_tz":-330},"id":"uw6luxJxaR2t","outputId":"95d1defc-2cab-4b3c-8eb9-b026fbcdeaba"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting confluent_kafka\n","  Downloading confluent_kafka-1.8.2-cp37-cp37m-manylinux2010_x86_64.whl (2.8 MB)\n","\u001b[K     |████████████████████████████████| 2.8 MB 26.1 MB/s \n","\u001b[?25hInstalling collected packages: confluent-kafka\n","Successfully installed confluent-kafka-1.8.2\n"]}],"source":["# Confluent (Open Source) is a developer-optimized distribution of Apache Kafka.\n","# Confluent is a more complete distribution of Apache Kafka. It expands Kafka’s integration capabilities, \n","# adding tools to optimize and manage Kafka clusters and methods to ensure the streams are secure. \n","# It makes Kafka easier to build and easier to operate\n","\n","!pip install confluent_kafka"]},{"cell_type":"markdown","metadata":{"id":"87xlk-ydYANy"},"source":["### Connect to Kafka cluster"]},{"cell_type":"markdown","metadata":{"id":"3XBDe2cxYANz"},"source":["CloudKarafka provides Apache Kafka as a service and offers tools to simplify the usage of it.\n","\n","To know more about CloudKarafka click [here](https://www.cloudkarafka.com/docs/productoverview.html).\n","\n","**CloudKarafka login:** Login to [Cloudkarafka](https://www.cloudkarafka.com) and create an instance\n","\n","For detailed instructions on the account and instance creation, please refer to this [document](https://cdn.iisc.talentsprint.com/CDS/CloudKarafka.pdf).\n","\n","**Connect the cluster:**\n","\n","* Create an instance and get credentials\n","* Create two topics (one for each example) and note down the topic names"]},{"cell_type":"markdown","metadata":{"id":"xoryOW8zz8vq"},"source":["Specify your `BROKERS`, `USERNAME`, `PASSWORD`, and `TOPIC` in the below script files."]},{"cell_type":"markdown","metadata":{"id":"VMO_ueoU3VwY"},"source":["### Example 1: Send and receive messages"]},{"cell_type":"markdown","metadata":{"id":"1DJkNkch5v1c"},"source":["Here we create two files one is `producer1.py` and another one is `consumer1.py`(in Consumer notebook). Producer will send messages to a topic and consumer will read these messages in real-time from that particular topic and displays the message along with its word count and an alert message if the number of words exceeds 6."]},{"cell_type":"markdown","metadata":{"id":"0WOc_3FQSeZf"},"source":["#### Write Producer file"]},{"cell_type":"markdown","metadata":{"id":"Pe9bHDY8V2Ge"},"source":["Here the producer will send messages to the specified `topic`."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":424,"status":"ok","timestamp":1644061344283,"user":{"displayName":"shambhavi seth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh20oin3w2PRiKUhrvgT0XoIOjlTZOhPgvWiKgfFNg=s64","userId":"04318899503633324567"},"user_tz":-330},"id":"-lHkW4aWEiuF","outputId":"14d0b862-2e5c-43e4-c74d-01dde417aa84"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing producer1.py\n"]}],"source":["%%writefile producer1.py\n","\n","import sys\n","import os\n","\n","from confluent_kafka import Producer\n","\n","# Specify BROKERS, USERNAME, PASSWORD and TOPIC\n","brokers = \"tricycle-01.srvs.cloudkafka.com:9094,tricycle-02.srvs.cloudkafka.com:9094,tricycle-03.srvs.cloudkafka.com:9094\" \n","username = \"dgcdj8l4\"\n","password = \"RoVwni79jy5zootYD7nCiVZXv0DexzQI\"\n","topic = \"dgcdj8l4-default\"\n","\n","# Set the path for the user-defined modules so that they can be directly imported into the python program\n","os.environ['CLOUDKARAFKA_BROKERS']= brokers\n","os.environ['CLOUDKARAFKA_USERNAME']= username\n","os.environ['CLOUDKARAFKA_PASSWORD']= password\n","os.environ['CLOUDKARAFKA_TOPIC']= topic\n","\n","if __name__ == '__main__':\n","    topic = os.environ['CLOUDKARAFKA_TOPIC'].split(\",\")[0]\n","\n","    # Consumer configuration\n","    conf = {\n","        'bootstrap.servers': os.environ['CLOUDKARAFKA_BROKERS'],                # Specify kafka servers\n","        'session.timeout.ms': 6000,                                             # The producer sends periodic heartbeats to indicate its liveness to the broker\n","        'default.topic.config': {'auto.offset.reset': 'smallest'},              # if there is no offset info, the offset will be set to the smallest value available\n","        'security.protocol': 'SASL_SSL',                                        # protocol used to communicate with brokers\n","\t      'sasl.mechanisms': 'SCRAM-SHA-256',                       # SASL mechanism to use for authentication. Supported: GSSAPI, PLAIN, SCRAM-SHA-256, SCRAM-SHA-512, OAUTHBEARER        \n","        'sasl.username': os.environ['CLOUDKARAFKA_USERNAME'],\n","        'sasl.password': os.environ['CLOUDKARAFKA_PASSWORD']\n","    }\n","\n","    p = Producer(**conf)\n","\n","    def delivery_callback(err, msg):\n","        if err:\n","            sys.stderr.write('%% Message failed delivery: %s\\n' % err)\n","        else:\n","            sys.stderr.write('%% Message delivered to %s [%d]\\n' %(msg.topic(), msg.partition()))\n","    print(\"\\nEnter text: \")\n","    # Take input data continuously\n","    for line in sys.stdin:                             \n","        try:\n","            # send data to specified topic\n","            p.produce(topic, line.rstrip(), callback=delivery_callback)       \n","        except BufferError as e:\n","            sys.stderr.write('%% Local producer queue is full (%d messages awaiting delivery): try again\\n' %len(p))\n","        p.poll(0)\n","        print(\"\\nEnter text or interupt the execution to stop.\")\n","\n","    sys.stderr.write('%% Waiting for %d deliveries\\n' % len(p))\n","    p.flush()            # makes all buffered records immediately available to send"]},{"cell_type":"markdown","metadata":{"id":"R_gOm4EA3uK5"},"source":["#### Run Producer file"]},{"cell_type":"markdown","metadata":{"id":"nyjb8u7m3uK6"},"source":["Before running the producer file, make sure that the corresponding consumer file `consumer1.py` is running in [Consumer notebook](https://drive.google.com/file/d/1xX0DA_QsDQeCnYtLklZVE2G9ke0gs41R/view?usp=sharing).\n","\n","The producer will keep on running and allow us to send messages. The output will be shown on the consumer side.\n","\n","<font color='blue'>Before executing the below cell ensure that you created the CloudKarafka account and specified the credentials.</font>"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":498904,"status":"ok","timestamp":1644062432542,"user":{"displayName":"shambhavi seth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh20oin3w2PRiKUhrvgT0XoIOjlTZOhPgvWiKgfFNg=s64","userId":"04318899503633324567"},"user_tz":-330},"id":"Zh-zwEuw61XM","outputId":"d9970f18-c99a-4145-d459-558690f34184"},"outputs":[{"name":"stdout","output_type":"stream","text":["%4|1644061938.739|CONFWARN|rdkafka#producer-1| [thrd:app]: Configuration property session.timeout.ms is a consumer property and will be ignored by this producer instance\n","%4|1644061938.739|CONFWARN|rdkafka#producer-1| [thrd:app]: Configuration property auto.offset.reset is a consumer property and will be ignored by this producer instance\n","\n","Enter text: \n","lets try y gettig a word with a log legth to tes this\n","\n","Enter text or interupt the execution to stop.\n","check me\n","% Message delivered to dgcdj8l4-default [3]\n","\n","Enter text or interupt the execution to stop.\n","example 2\n","% Message delivered to dgcdj8l4-default [4]\n","\n","Enter text or interupt the execution to stop.\n","example 3\n","% Message delivered to dgcdj8l4-default [0]\n","\n","Enter text or interupt the execution to stop.\n","Traceback (most recent call last):\n","  File \"producer1.py\", line 44, in <module>\n","KeyboardInterrupt\n"]}],"source":["!python producer1.py"]},{"cell_type":"markdown","metadata":{"id":"NftTA06UXtlF"},"source":["For next example **create a new topic** on CloudKarafka and use its topic name. To create a topic, please refer to step 11 in this [document](https://cdn.iisc.talentsprint.com/CDS/CloudKarafka.pdf)."]},{"cell_type":"markdown","metadata":{"id":"Na97-y5N1kSm"},"source":["### Example 2: Compute the rolling mean of the last three insertions"]},{"cell_type":"markdown","metadata":{"id":"PAP5mcrmUwxG"},"source":["Here we create two files one is `producer2.py` and other one is `consumer2.py`(in Consumer notebook). Producer will send data to a topic and consumer will read these records in real-time from that particular topic and displays the rolling mean of the last three insertions. Only the added numbers will be displayed for the first two insertions."]},{"cell_type":"markdown","metadata":{"id":"bS6rbvP536cU"},"source":["#### Write Producer file"]},{"cell_type":"markdown","metadata":{"id":"V_hBqISN36cW"},"source":["Here the producer will send messages to the specified `topic`."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":407,"status":"ok","timestamp":1644062866781,"user":{"displayName":"shambhavi seth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh20oin3w2PRiKUhrvgT0XoIOjlTZOhPgvWiKgfFNg=s64","userId":"04318899503633324567"},"user_tz":-330},"id":"dK4jpidFfj4n","outputId":"9b4d6bda-fda1-4fe0-f4e8-336231b6c4f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing producer2.py\n"]}],"source":["%%writefile producer2.py\n","\n","import sys\n","import os\n","\n","from confluent_kafka import Producer\n","\n","# Specify BROKERS, USERNAME, PASSWORD and new TOPIC\n","brokers = \"\" \n","username = \"\"\n","password = \"\"\n","topic = \"\"\n","\n","# Set the path for the user-defined modules so that they can be directly imported into the python program\n","os.environ['CLOUDKARAFKA_BROKERS']= brokers\n","os.environ['CLOUDKARAFKA_USERNAME']= username\n","os.environ['CLOUDKARAFKA_PASSWORD']= password\n","os.environ['CLOUDKARAFKA_TOPIC']= topic\n","\n","if __name__ == '__main__':\n","    \n","    topic = os.environ['CLOUDKARAFKA_TOPIC'].split(\",\")[0]\n","\n","    # Consumer configuration\n","    conf = {\n","        'bootstrap.servers': os.environ['CLOUDKARAFKA_BROKERS'],                # Specify kafka servers\n","        'session.timeout.ms': 6000,                                             # The producer sends periodic heartbeats to indicate its liveness to the broker\n","        'default.topic.config': {'auto.offset.reset': 'smallest'},              # if there is no offset info, the offset will be set to the smallest value available\n","        'security.protocol': 'SASL_SSL',                                        # protocol used to communicate with brokers\n","\t      'sasl.mechanisms': 'SCRAM-SHA-256',                       # SASL mechanism to use for authentication. Supported: GSSAPI, PLAIN, SCRAM-SHA-256, SCRAM-SHA-512, OAUTHBEARER        \n","        'sasl.username': os.environ['CLOUDKARAFKA_USERNAME'],\n","        'sasl.password': os.environ['CLOUDKARAFKA_PASSWORD']\n","    }\n","\n","    \n","    p = Producer(**conf)\n","\n","    def delivery_callback(err, msg):\n","        if err:\n","            sys.stderr.write('%% Message failed delivery: %s\\n' % err)\n","        else:\n","            sys.stderr.write('%% Message delivered to %s [%d]\\n' %(msg.topic(), msg.partition()))\n","    print(\"\\nEnter number: \")\n","    # Take input data continuously\n","    for line in sys.stdin:                           \n","        try:\n","            # send data to specified topic\n","            p.produce(topic, line, callback=delivery_callback)                 \n","        except BufferError as e:\n","           \n","            sys.stderr.write('%% Local producer queue is full (%d messages awaiting delivery): try again\\n' %len(p))\n","\n","        p.poll(0)\n","        print(\"\\nEnter a number or interupt the execution to stop.\")\n","\n","    sys.stderr.write('%% Waiting for %d deliveries\\n' % len(p))\n","\n","    p.flush()                     # makes all buffered records immediately available to send"]},{"cell_type":"markdown","metadata":{"id":"kPLtUEC6SiEo"},"source":["#### Run Producer file"]},{"cell_type":"markdown","metadata":{"id":"V_kHIE4dUT0H"},"source":["Before running the producer file, please make sure that the corresponding consumer file `consumer2.py` is running in [Consumer notebook](https://drive.google.com/file/d/1xX0DA_QsDQeCnYtLklZVE2G9ke0gs41R/view?usp=sharing).\n","\n","The producer will keep on running and allow us to send messages. The output will be shown on the consumer side.\n","\n","<font color='blue'>Before executing the below cell ensure that you created the CloudKarafka account and specified the credentials.</font>"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":298089,"status":"ok","timestamp":1644063367486,"user":{"displayName":"shambhavi seth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh20oin3w2PRiKUhrvgT0XoIOjlTZOhPgvWiKgfFNg=s64","userId":"04318899503633324567"},"user_tz":-330},"id":"b7689U76fpn3","outputId":"f05f321f-b888-4b70-cce7-540b29e25062"},"outputs":[{"name":"stdout","output_type":"stream","text":["%4|1644063074.499|CONFWARN|rdkafka#producer-1| [thrd:app]: Configuration property session.timeout.ms is a consumer property and will be ignored by this producer instance\n","%4|1644063074.499|CONFWARN|rdkafka#producer-1| [thrd:app]: Configuration property auto.offset.reset is a consumer property and will be ignored by this producer instance\n","\n","Enter number: \n","1\n","\n","Enter a number or interupt the execution to stop.\n","2\n","\n","Enter a number or interupt the execution to stop.\n","3\n","\n","Enter a number or interupt the execution to stop.\n","4\n","% Message delivered to dgcdj8l4-topic1 [4]\n","\n","Enter a number or interupt the execution to stop.\n","5\n","\n","Enter a number or interupt the execution to stop.\n","6\n","\n","Enter a number or interupt the execution to stop.\n","7\n","\n","Enter a number or interupt the execution to stop.\n","8\n","% Message delivered to dgcdj8l4-topic1 [3]\n","% Message delivered to dgcdj8l4-topic1 [0]\n","% Message delivered to dgcdj8l4-topic1 [0]\n","\n","Enter a number or interupt the execution to stop.\n","9\n","\n","Enter a number or interupt the execution to stop.\n","%5|1644063150.563|REQTMOUT|rdkafka#producer-1| [thrd:sasl_ssl://tricycle-02.srvs.cloudkafka.com:9094/bootstrap]: sasl_ssl://tricycle-02.srvs.cloudkafka.com:9094/1: Timed out MetadataRequest in flight (after 60063ms, timeout #0)\n","%4|1644063150.563|REQTMOUT|rdkafka#producer-1| [thrd:sasl_ssl://tricycle-02.srvs.cloudkafka.com:9094/bootstrap]: sasl_ssl://tricycle-02.srvs.cloudkafka.com:9094/1: Timed out 1 in-flight, 0 retry-queued, 0 out-queue, 0 partially-sent requests\n","%3|1644063150.563|FAIL|rdkafka#producer-1| [thrd:sasl_ssl://tricycle-02.srvs.cloudkafka.com:9094/bootstrap]: sasl_ssl://tricycle-02.srvs.cloudkafka.com:9094/1: 1 request(s) timed out: disconnect (after 75981ms in state UP)\n","15\n","% Message delivered to dgcdj8l4-topic1 [4]\n","% Message delivered to dgcdj8l4-topic1 [1]\n","% Message delivered to dgcdj8l4-topic1 [4]\n","% Message delivered to dgcdj8l4-topic1 [2]\n","% Message delivered to dgcdj8l4-topic1 [2]\n","\n","Enter a number or interupt the execution to stop.\n","Traceback (most recent call last):\n","  File \"producer2.py\", line 48, in <module>\n","    for line in sys.stdin:                           \n","KeyboardInterrupt\n","^C\n"]}],"source":["\n","!python producer2.py"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"M5_AST_07_Kafka_Streaming_Producer_A.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}
