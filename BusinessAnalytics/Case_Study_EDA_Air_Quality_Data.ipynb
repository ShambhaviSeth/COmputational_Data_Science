{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"M6_NB_Case_Study_EDA_Air_Quality_Data.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"X36mJe9JoFQ5"},"source":["# Advanced Certification Program in Computational Data Science\n","## A program by IISc and TalentSprint\n","### Case Study: EDA Air Quality Data"]},{"cell_type":"markdown","metadata":{"id":"RyUt8idMbjO9"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"CrHoiYuHbkOh"},"source":["At the end of the case study, you will be able to\n","\n","* understand various steps in exploratory data analysis\n","* implement those steps on Beijing air quality data which is a time series data\n","* know how to gain insights using these steps as a part of exploratory data analysis "]},{"cell_type":"markdown","metadata":{"id":"3R9A_YqnprD3"},"source":["## Information"]},{"cell_type":"markdown","metadata":{"id":"Xqm36v6eqeLp"},"source":["#### Dataset Description"]},{"cell_type":"markdown","metadata":{"id":"iTkh71XXt_3D"},"source":["In this assignment tutorial, we will be working on 'Beijing's Air Quality Dataset' which is a time series dataset of hourly air quality.\n","\n","This data set includes hourly air pollutants data from a nationally-controlled air-quality monitoring site (Dingling Station). The air-quality data are from the Beijing Municipal Environmental Monitoring Center. The meteorological data in each air-quality site are matched with the nearest weather station from the China Meteorological Administration. The time period is from March 1st, 2013 to February 28th, 2017. Missing data are denoted as NAN.\n","\n","**Attribute Information**\n","\n","1. No: row number\n","2. year: year of data in this row\n","3. month: month of data in this row\n","4. day: day of data in this row\n","5. hour: hour of data in this row\n","6. PM2.5: PM2.5 concentration (ug/m^3)\n","7. PM10: PM10 concentration (ug/m^3)\n","8. SO2: SO2 concentration (ug/m^3)\n","9. NO2: NO2 concentration (ug/m^3)\n","10. CO: CO concentration (ug/m^3)\n","11. O3: O3 concentration (ug/m^3)\n","12. TEMP: temperature (degree Celsius)\n","13. PRES: pressure (hPa)\n","14. DEWP: dew point temperature (degree Celsius)\n","15. RAIN: precipitation (mm)\n","16. wd: wind direction\n","17. WSPM: wind speed (m/s)\n","18. station: name of the air-quality monitoring site\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fFnKWukisE59"},"source":["### Importing required packages"]},{"cell_type":"code","metadata":{"id":"Nz7WgZLic85K"},"source":["# A tool/package to download files from the web\n","!pip -q install download                                 "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CUcf_5tAeG60"},"source":["' filesystem_spec, a specification for pythonic filesystems, to produce a template or specification for a file-system interface '\n","!pip -q install fsspec                                                   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k726sqKBnwPh"},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","import numpy as np\n","import pandas as pd\n","import os\n","import seaborn as sns\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","from datetime import datetime\n","import plotly.express as px\n","from download import download\n","from sklearn.impute import SimpleImputer\n","mpl.rcParams['figure.figsize'] = (8, 6)\n","mpl.rcParams['axes.grid'] = False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lrpyxH9lqXEG"},"source":["### EDA Explained With Time Series Dataset"]},{"cell_type":"code","metadata":{"cellView":"form","id":"HN5wm2bVEWBg"},"source":["#@title Download Dataset\n","!wget -q https://cdn.iisc.talentsprint.com/CDS/Datasets/PRSA_Data_Dingling_20130301-20170228.csv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DgpRk3T3blHj"},"source":["# reading the .csv file of one monitoring station (Dingling, Changping District) of Beijing\n","df = pd.read_csv('PRSA_Data_Dingling_20130301-20170228.csv', encoding='ISO-8859-1')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6j147jtnfKny"},"source":["# displaying the data\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r1G_W4drfsZ1"},"source":["So, from above, we can say that it is time-series data. The data is from 1st March 2013 to 28th February 2017. From the `hour column`, it is clearly seen that the data is recorded for every hour of the day. So, the data is ordered at the time. Here, we are taking the target variable as $PM_{2.5}$ as it is the key pollutant that affects the respiratory health of the ambient environment. Other variables are also there, which include other pollutants and meteorological parameters."]},{"cell_type":"code","metadata":{"id":"yEno2qhPfNqb"},"source":["# data information \n","df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RmuKwTNTfpik"},"source":["# defining a function for date time\n","def convert_to_date(x):\n","  return datetime.strptime(x, '%Y %m %d %H')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oW9e1a2XhdKn"},"source":["aq_df = pd.read_csv('PRSA_Data_Dingling_20130301-20170228.csv', parse_dates=[['year', 'month', 'day', 'hour']], date_parser=convert_to_date, keep_date_col=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"scWJHt7bh06D"},"source":["# first five rows of the dataset\n","aq_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tkgezg2Ih6Bx"},"source":["So, we created a single `concatenate` column which consists of year, month, date, and time, respectively. It is convenient that we make such a column in the dataset because with that, we can take any value within a particular time interval using the `pandas .iloc` function, which we will see in next sections."]},{"cell_type":"code","metadata":{"id":"80m3t_G6h32S"},"source":["aq_df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YpOreEg-jz5g"},"source":["Now, we convert `month` to numeric for monthly basis data analysis."]},{"cell_type":"code","metadata":{"id":"h-3uv7JXjw1t"},"source":["# converting month to numeric value data\n","aq_df['month']=pd.to_numeric(aq_df['month'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lJ5J_V1mk-oc"},"source":["Now, we are going to print the number of rows, the number of columns (or features), the number of missing values, and the number of unique values in our dataset."]},{"cell_type":"code","metadata":{"id":"Dgv7qBjmk63t"},"source":["# diplaying the number of rows, columns, features, missing/null values and unique values in the dataset\n","print(\"Rows   :\", aq_df.shape[0])\n","print(\"Columns    :\", aq_df.shape[1])\n","print(\"\\nFeatures  :\", aq_df.columns.tolist())\n","print(\"\\nMissing values  :\", aq_df.isnull().any())\n","print(\"\\nUnique values  :\", aq_df.nunique())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DuEI97X2mbRR"},"source":["Thus, as of now, we have 19 columns, 35064 rows in our dataset. True and False indicates that the feature has missing values and below that the number of unique values in each feature is shown."]},{"cell_type":"code","metadata":{"id":"rwYq7IEJmLC9"},"source":["# descriptive statistics\n","aq_df.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0jkuLUj_nBgV"},"source":["From above, we can make basic interpretations of data by seeing standard deviation, mean, minimum, maximum, etc., values. E.g., the temperature in the city can take a minimum value of -16 and a maximum value of 41, which suggests that the summers are moderately hot and the winters are too cold."]},{"cell_type":"code","metadata":{"id":"ut6kcM0rm9XT"},"source":["# making a copy of data unindexed\n","aq_df_non_index= aq_df.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0gJHUys7n_Zi"},"source":["# setting index as yy mm dd hh format\n","aq_df = aq_df.set_index('year_month_day_hour')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O8QUSz2WoLRt"},"source":["aq_df.index"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vEPPaMIdoR-V"},"source":["Hence, we set our index as the date time stamp as a lot of plots for data visualization require, data indexing. Also, for subsetting and filtering data indexing is required.\n","\n"]},{"cell_type":"code","metadata":{"id":"IlVBSQLdoNXN"},"source":["# first five rows \n","aq_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6MLfslonoYkS"},"source":["# taking values from 1st march, 2013 to 5th march, 2013\n","aq_df.loc['2013-03-01':'2013-03-05']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8FlnSunVpVbV"},"source":["# taking values from 2013 to 2015 ,i.e., two year of data\n","aq_df.loc['2013':'2015']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6C3JZVOgp3IN"},"source":["**Note:** We can do the above operations using pandas functions also without indexing. However, indexing makes this kind of operation fast and efficient. Also, some of the functions that we used for plotting considers data time to plot the data.\n","\n","Now, in the next section, we will take $PM_{2.5}$ out of the data frame as we select it as our target variable."]},{"cell_type":"code","metadata":{"id":"FlbIsONWptae"},"source":["# taking PM2.5 from the data frame\n","pm_dt = aq_df['PM2.5']\n","pm_dt.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a7F4GN5Tq34M"},"source":["# plotting PM2.5 data from 2013 to 2017\n","pm_dt.plot(grid=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CXmDV6dTrU8P"},"source":["Since there are a lot of data points, we cannot infer the graph here. Still, we can say that at the beginning and end of every year the spikes increase.\n","\n","Now, we take only one year to visualize $PM_{2.5}$."]},{"cell_type":"code","metadata":{"id":"7Z1PJUxdq-xV"},"source":["# taking the year 2015\n","aq_df_2015 = aq_df.loc['2015']\n","# taking PM2.5 from above data made\n","pm_dt_2015 = aq_df_2015['PM2.5']\n","# plotting variation of PM2.5 for the year 2015\n","pm_dt_2015.plot(grid=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kSOP7xuusFc_"},"source":["So, from the above plot, we can infer that the $PM_{2.5}$ levels are high in Jan, Feb, Mar, and after that, a decreasing trend can be seen, which again increases in Oct, Nov, and Dec.\n","\n","However, is this pattern repeating every year? We can see that by plotting for the year 2014."]},{"cell_type":"code","metadata":{"id":"G28idYI1sBA0"},"source":["# taking the year 2014\n","aq_df_2014 = aq_df.loc['2014']\n","pm_dt_2014 = aq_df_2015['PM2.5']\n","pm_dt_2014.plot(grid=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qOiO1aXbtI-l"},"source":["Thus, we can see that in 2014 also almost the same trend of $PM_{2.5}$ levels is appearing. \n","\n","From this, we can infer that there is some seasonality in the concentration trend of $PM_{2.5}$. In the month of Jan-Mar and Oct-Dec increase in the levels of $PM_{2.5}$ is recorded, and between them (Apr, Jun, Jul, Aug, Sept), there is a low level of $PM_{2.5}$ recorded. \n","\n","Thus, we can say that the concentration levels of $PM_{2.5}$ have some seasonality. Does it correlate with the winter season? We will see that in the following sections."]},{"cell_type":"markdown","metadata":{"id":"L30U6GJUu0-i"},"source":["The above plots are pretty tiresome. We need to plot individually for every year and between years to visualize our data. Is there a better method where we can plot all the data and do the individual analysis? Yes, we can do that.\n","\n","Python has a library for that which is `Plotly.express`."]},{"cell_type":"code","metadata":{"id":"uMXCqZvZtA6i"},"source":["fig = px.line(aq_df_non_index, x = 'year_month_day_hour', y = 'PM2.5', title='PM2.5 with slider')\n","\n","# visible the slider\n","fig.update_xaxes(rangeslider_visible=True)\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"taTefjTf9jjr"},"source":["Here, using the slider we can narrow down our data and visualize it rather than creating a separate plot for each year, the month of a year, days of a month, and hourly basis. This makes the visualization even more convenient."]},{"cell_type":"code","metadata":{"id":"Z_w1v1392hzo"},"source":["fig = px.line(aq_df_non_index, x = 'year_month_day_hour', y = 'PM2.5', title='PM2.5 with slider')\n","\n","fig.update_xaxes(\n","    rangeslider_visible=True,\n","    rangeselector=dict(\n","        buttons=list([\n","                      dict(count=1, label='y', step='year', stepmode='backward'),\n","                      dict(count=2, label='2y', step='year', stepmode='backward'),\n","                      dict(count=3, label='3y', step='year', stepmode='backward'),\n","                      dict(step='all')\n","        ])\n","    )\n",")\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oi80gfk2-fxr"},"source":["The above plot is the same, just only one difference here. We are providing buttons for years as 1 year (1y), 2 years (2y), and 3 years (3y). We can change these buttons to month or day by passing 'month' and 'day' instead of the year in `step` argument.\n","\n","Now, we are overlapping two years (2014 and 2015) data for $PM_{2.5}$ and visualize the resulting plot."]},{"cell_type":"code","metadata":{"id":"XuI5Adj99TfL"},"source":["df_2014 = aq_df['2014'].reset_index()\n","df_2015 = aq_df['2015'].reset_index()\n","# removing the hour using lambda function\n","df_2014['month_day_hour']=df_2014.apply(lambda x : str(x['month'])+\"-\"+x['day'], axis=1)\n","df_2015['month_day_hour']=df_2015.apply(lambda x : str(x['month'])+\"-\"+x['day'], axis=1)\n","plt.plot(df_2014['month_day_hour'], df_2014['PM2.5'])\n","plt.plot(df_2015['month_day_hour'], df_2015['PM2.5'])\n","plt.legend(['2014','2015'])\n","plt.xlabel('Month')\n","plt.ylabel('PM2.5')\n","plt.title('Air Quality plot of PM2.5 for the year 2014 and 2015')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ESvM5YvaMzEn"},"source":["So, we can see that the two plots overlaid on one another. This means that the variation of $PM_{2.5}$ is almost identical in both years. We can infer from here that the variation throughout the year for different years is identical. \n","\n","**Note:** The above plot is based on Month and Day level Variations for 2014 and 2015.\n","\n","Now, we will group the data by month in the next section."]},{"cell_type":"code","metadata":{"id":"abfiacKdBq5d"},"source":["aq_df['2014':'2016'][['month', 'PM2.5']].groupby('month').describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U_q1ropDRUd3"},"source":["So, from here also, we can see that the mean variation of $PM_{2.5}$ increases from Jan to Apr and then decreases and then again increases from Oct to Dec. This descriptive analysis supports the plot we made in the above sections."]},{"cell_type":"markdown","metadata":{"id":"XL9nWqWuR3Sm"},"source":["Now, we will group the variation of $PM_{2.5}$ with temperature. We are taking the maximum value of $PM_{2.5}$ and the minimum and maximum value of temperature. So, we are going from a univariate analysis of $PM_{2.5}$ to a bivariate analysis."]},{"cell_type":"code","metadata":{"id":"riucSdXtRR9T"},"source":["aq_df['2014':'2016'][['month', 'PM2.5', 'TEMP']].groupby('month').agg({'PM2.5':['max'], 'TEMP':['min', 'max']})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nSnkxEqcTBk7"},"source":["# taking 2015 data\n","aq_df_2015=aq_df['2015']\n","# PM2.5 and Temperature variation in 2015\n","pm_dt_2015=aq_df_2015[['PM2.5', 'TEMP']]\n","# plotting the variations\n","pm_dt_2015.plot(subplots=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6-UjhoqTUlkn"},"source":["From the above variations, it is confirmed that in the winter season Jan to Mar and from Oct to Dec the variation of $PM_{2.5}$ shows an increasing trend, and the high values of $PM_{2.5}$ concentration are seen and in between from Apr to Sept and the decreasing trend can be seen and very low levels of $PM_{2.5}$ is recorded.\n","\n","If we are from countries like China, Pakistan, India, Bangladesh, we can infer why this is happening. However, it will require domain knowledge.\n","\n","Here in the given [article](https://weather.com/en-IN/india/science/news/2018-10-30-why-do-pollution-levels-skyrocket-during-winter), you can find why $PM_{2.5}$  has such trend or seasonality in its variations."]},{"cell_type":"markdown","metadata":{"id":"nVglGSz6ZQy6"},"source":["Now, we plot a histogram for $PM_{2.5}$ and temperature and see how it looks like."]},{"cell_type":"code","metadata":{"id":"A29YznP3TufU"},"source":["aq_df[['PM2.5', 'TEMP']].hist()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3x3LuQC6ad9D"},"source":["So, we can see that in the case of $PM_{2.5}$ the frequency of lower concentration is very as compared to higher levels. \n","\n","Moreover, in temperature, we can see a bi-modal distribution plot, which means it has two spikes, one in winter and one during summers.\n","\n","So, to look at the temperature, we will plot a density curve to observe the two spikes."]},{"cell_type":"code","metadata":{"id":"zQp1G-dMZmg_"},"source":["aq_df['TEMP'].plot(kind='density')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VKcQfjATdLLh"},"source":["So clearly, we see two spikes, one at a lower temperature in the winter season and one at a higher temperature means in summers."]},{"cell_type":"markdown","metadata":{"id":"rhGOtVZXdaY7"},"source":["Now, we are going to plot the \"lag plots.\" Firstly, what are **LAG PLOTS**?\n","\n","The lag plot is a special type of scatter plot with two variables. The abscissa is the current time and the ordinate shows the lagged period. Now, by default, the lag period is 1, and it is called the first-order lag plot. When the lag period is 2 it is called the second-order lag plot and so on. Lag plots give much insight into our data. The linear shape of the lag plot suggests that an Autoregressive model is probably the better choice to model the data. The Lag plot can be used in multiple cases like checking the linearity in the data, checking the outliers, we can check randomness in the data, and also if there is a serial correlation or autocorrelation.\n","\n","Let us plot different order lag plots now.\n","\n"]},{"cell_type":"code","metadata":{"id":"W0TDoWIrczG2"},"source":["# first order lag plot\n","# here our data is collected on the hourly basis\n","# so lag = 1 means the plot between current hour and preceding hour\n","pd.plotting.lag_plot(aq_df['TEMP'], lag=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HS7P7iBFiUXt"},"source":["Here, we can see that there is a linear correlation, which is also justifiable as the temperature cannot change much on an hourly basis mostly."]},{"cell_type":"code","metadata":{"id":"bl2EY7WXfMfK"},"source":["pd.plotting.lag_plot(aq_df['TEMP'], lag=10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l0XO-VMGiqFf"},"source":["So, after every 10 hour, there is a serious change in the temperature and that's why there is no correlation seen here."]},{"cell_type":"code","metadata":{"id":"PdXg1druihth"},"source":["pd.plotting.lag_plot(aq_df['TEMP'], lag=24)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZKB3mC03jMJo"},"source":["After lagging 24 hours means one day, we can observe that the data is correlated but not like that in 1-hour lag. Moreover, we can see an increasing order plot means the autocorrelation is positive here (the data is going from bottom left to top right). \n","\n","Note that here we are talking about **autocorrelation**, which means correlation within itself. Correlation typically means the relationship between two variables like spearman and Pearson's correlation which we will see further in the next sections.\n","\n","One more important use case of **Lag Plot** is to observe stationarity in the data. Whether the data is stationary or not.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cRfyyOxflx0J"},"source":["Now, we will plot the lag plot for one year.\n","\n","Here we will take 24*365 = 8640 lag period. Meaning, 24 hours for a day and 365 for the number of days in a year."]},{"cell_type":"code","metadata":{"id":"19p3mtIai2w3"},"source":["pd.plotting.lag_plot(aq_df['TEMP'], lag=8640)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7qDWU8zhmHQY"},"source":["We can see that the data is correlated, the data is spread out but still, it is a centered data. Thus, we can say that the temperature variation in trend is the same during the year."]},{"cell_type":"code","metadata":{"id":"a6OkyAnPmFW0"},"source":["# half yearly variation plot of temperature\n","pd.plotting.lag_plot(aq_df['TEMP'], lag=4320)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fxs10mZLmu93"},"source":["Here, we can observe that the data is negatively correlated which is justifiable. As the temperature trend changes from summer to winter."]},{"cell_type":"code","metadata":{"id":"3a9AB2_Jmth0"},"source":["# quaterly variation\n","pd.plotting.lag_plot(aq_df['TEMP'], lag=2160)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6tY5-FB4qayA"},"source":["After every quarter of a year, the temperature changes a lot. Thus, there is no correlation seen here."]},{"cell_type":"markdown","metadata":{"id":"DKO4qOJ4reyW"},"source":["### Multivariate plot\n","\n","Let's move from bivariate to multivariate plot. Here, along with temperature we will now add pressure."]},{"cell_type":"code","metadata":{"id":"bbVngkoaqYpd"},"source":["pm_dt_2015 = aq_df_2015[['PM2.5', 'TEMP', 'PRES']]\n","pm_dt_2015.plot(subplots=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HGMqX8alsdlC"},"source":["Here, we can see that, as temperature is more pressure is less means they both are negatively correlated. "]},{"cell_type":"code","metadata":{"id":"OsSLIneAsFQK"},"source":["# plotting multiple weather related variables with PM2.5\n","multi_data = aq_df[['PM2.5', 'TEMP', 'PRES', 'DEWP', 'RAIN']]\n","multi_data.plot(subplots=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YtqNDzsLuoKZ"},"source":["Now, we will plot the other pollutants along with $PM_{2.5}$."]},{"cell_type":"code","metadata":{"id":"HKefVPhItuqV"},"source":["multi_data=aq_df[['SO2','NO2','O3','CO','PM2.5']]\n","multi_data.plot(subplots=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pNRSZdNMvdOk"},"source":["We can see that $PM_{2.5}$ and $CO$ having almost the same trend in variation and $PM_{2.5}$ and $O_3$ have opposite trend in variation. Means, in times when $PM_{2.5}$ levels are high that time $O_{3}$ levels are quite low and vice-versa."]},{"cell_type":"code","metadata":{"id":"Li8pgtRju92u"},"source":["aq_df['2014':'2015'][['PM2.5', 'O3']].plot(figsize=(13,6.5), linewidth=3, fontsize=14)\n","plt.xlabel('year_month_day_hour', fontsize=20);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"22zNSboIwwDK"},"source":["Here, we can clearly infer the type of relation between $PM_{2.5}$ and $O_3$ trends. In months where  $PM_{2.5}$ levels are high $O_3$ levels are low and vice-versa."]},{"cell_type":"markdown","metadata":{"id":"C5a9NZuvxkWk"},"source":["### Checking and Imputing Null Values in the data\n","\n","If the data has null values in it, we can either drop them or impute them. In the case of time series analysis, we cannot drop the null value data point. This is because if we drop those data points, the continuity of our data is hindered. So, the idea is to impute the null values with a standard value of the observation. There are statistical observations like mean value, median values, mode value, etc., by which we can impute null values and depend on the type of problem we are dealing with."]},{"cell_type":"code","metadata":{"id":"wf1_6F3Jwns5"},"source":["# observing data of the year 2015\n","aq_df_2015"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KYGxc9jN0qTu"},"source":["The **NAN** values in the dataset represent the null values. This may happen because sometimes the sensor or the recording station is not working or under maintenance so there is no observation recorded for that particular day or time."]},{"cell_type":"code","metadata":{"id":"61N9zUx30bls"},"source":["# this gives true or false\n","# true: means null values present, false: null values absent\n","aq_df.isnull().values.any()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zyy4ukOR1r6a"},"source":["# the total count of null values in each observation\n","aq_df.isnull().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cSNT4uqq2VMW"},"source":["#### Imputing Null/Missing Values"]},{"cell_type":"code","metadata":{"id":"dLjNA4LWAWZS"},"source":["df_1 = aq_df_2015.drop(['wd', 'station'], axis = 1)\n","df_1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IDvJSWCJ-4dN"},"source":["# example of imputing missing values using Scikit-Learn\n","\n","# retrieve the numpy array\n","values = df_1.values\n","# define the imputer\n","imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n","# transform the dataset\n","transformed_values = imputer.fit_transform(values)\n","\n","df_1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-bvYh6GcCnAw"},"source":["### Correlation plot between Features"]},{"cell_type":"code","metadata":{"id":"ID6sVXo2B2tA"},"source":["g = sns.pairplot(aq_df[['SO2','NO2','O3','CO', 'PM2.5']])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A3dadRytDO1r"},"source":["By this, we can observe the correlation between the features.\n","\n","We can also find the values of correlation by using the pearson correlation matrix."]},{"cell_type":"code","metadata":{"id":"r9KJ9taAC8Ea"},"source":["aq_pear_corr = aq_df[['SO2','NO2','O3','CO','PM10', 'PM2.5']].corr(method='pearson')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uvPfvpoMDunx"},"source":["aq_pear_corr"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2xF7E2zbD6dn"},"source":["Thus, we can interpret that $PM_{2.5}$ is highly correlated with $CO$ and $PM_{10}$ and moderately correlated with $NO_{2}$.\n","\n","For better observation we can  plot a heatmap."]},{"cell_type":"code","metadata":{"id":"q_0-YyxXDxCX"},"source":["g = sns.heatmap(aq_pear_corr, vmax=6, center=0,\n","                square=True, linewidths=0.5, cbar_kws={'shrink': 0.5}, annot=True, fmt='0.2f', cmap='coolwarm')\n","g.figure.set_size_inches(10, 10)\n","\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gs1ry1iqF2qq"},"source":["### Checking for outliers box plot"]},{"cell_type":"markdown","metadata":{"id":"v1YfYs6IJk5_"},"source":["A box plot is a method for graphically depicting groups of numerical data through their quartiles. The box extends from the Q1 to Q3 quartile values of the data, with a line at the median (Q2). The whiskers extend from the edges of box to show the range of the data. The position of the whiskers is set by default to 1.5 * IQR (IQR = Q3 - Q1) from the edges of the box. Outlier points are those past the end of the whiskers.\n","\n","Any data point outside this range is considered as outlier and should be removed for further analysis.\n","\n","The concept of quartiles and IQR can best be visualized from the boxplot. It has the minimum and maximum points defined as Q1–1.5*IQR and Q3+1.5*IQR respectively. Any point outside this range is outlier.\n","\n","![Image](https://miro.medium.com/max/481/1*8Eg6rRwfCNgXbY7ZX4C1ZA.png)"]},{"cell_type":"code","metadata":{"id":"mdySJWFqFYk1"},"source":["boxplot = aq_df_2015.boxplot(column=['O3','PM10', 'PM2.5'], grid=False, rot=45, fontsize=15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ix6LWJNWHkQW"},"source":["boxplot = aq_df_2015.boxplot(column=['SO2', 'NO2'], grid=False, rot=45, fontsize=15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o0JvRYrMHECV"},"source":["boxplot = aq_df_2015.boxplot(column=['O3'], grid=False, rot=45, fontsize=15)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HNABwD4eH2S2"},"source":["So, we can say that outliers are present in our dataset."]},{"cell_type":"markdown","metadata":{"id":"SC1I4A51TvCu"},"source":["#### Skewness Method to detect outliers\n","\n","Several machine learning algorithms make the assumption that the data follow a normal (or Gaussian) distribution. This is easy to check with the skewness value, which explains the extent to which the data is normally distributed. Ideally, the skewness value should be between -1 and +1, and any major deviation from this range indicates the presence of extreme values.\n","\n","The first line of code below prints the skewness value for the 'Pollution' variables, while the second line prints the summary statistics."]},{"cell_type":"code","metadata":{"id":"Tlv2PlxgT8rs"},"source":["# displaying skewness in features for the year 2015 \n","print(aq_df_2015.skew())\n","aq_df_2015.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N75KWH0GV_A8"},"source":["We can see that some features like $PM_{2.5}$, $PM_{10}$, $SO_2$, $NO_2$, $CO$, $O_3$, $RAIN$ and $WSPM$, having the value of *skewness* not in the optimal range, *indicating outliers*."]},{"cell_type":"markdown","metadata":{"id":"qCV4QTYuTtNj"},"source":["There are techniques to treat those outliers, one such technique is called IQR treatment of outliers will discuss in next section."]},{"cell_type":"markdown","metadata":{"id":"aDUBIlJaJCB2"},"source":["#### IQR Treatment of Outliers"]},{"cell_type":"markdown","metadata":{"id":"craXDAvJJCOc"},"source":["Each dataset can be divided into quartiles. The first quartile point indicates that 25% of the data points are below that value whereas second quartile is considered as median point of the dataset. The interquartile method finds the outliers on numerical datasets by following the procedure below:\n","\n","1. Find the first quartile, Q1.\n","2. Find the third quartile, Q3.\n","3. Calculate the IQR. IQR= Q3-Q1.\n","3. Define the normal data range with lower limit as Q1–1.5*IQR and upper limit as Q3+1.5*IQR.\n","\n"]},{"cell_type":"code","metadata":{"id":"B1jVpOM-L8DH"},"source":["# Finding outliers\n","Q1 = aq_df_2015.quantile(0.25)\n","Q3 = aq_df_2015.quantile(0.75)\n","IQR = Q3 - Q1\n","print(IQR)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x6XnCehSNgi-"},"source":["The above output prints the IQR scores, which can be used to detect outliers. The code below generates an output with the 'True' and 'False' values. Points where the values are 'True' represent the presence of the outlier."]},{"cell_type":"code","metadata":{"id":"eimDbUqWL_oa"},"source":["# removing outliers using IQR values and redefining the dataset\n","df_out = aq_df_2015[~((aq_df_2015 < (Q1 - 1.5 * IQR)) |(aq_df_2015 > (Q3 + 1.5 * IQR))).any(axis=1)]\n","print(df_out.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yZ0IujiWSn5y"},"source":["Here, we can see that the number of data points gets reduce from 8760 to 6031, meaning that we treat our data for outliers using IQR method. "]},{"cell_type":"markdown","metadata":{"id":"AkrFwmE0TRci"},"source":["Now, we will check the **skew values** and see if the skewness of our features decreases or not after outlier treatment."]},{"cell_type":"code","metadata":{"id":"kRruDcDjTJB1"},"source":["print(df_out.skew())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"63OzoQX7Thvs"},"source":["Thus, we can say that we are able to reduce the skewness in our data as what we are getting when the data is having outliers in it."]},{"cell_type":"markdown","metadata":{"id":"FMVQqqmyXWMB"},"source":["### Autocorrelation Plot\n","\n","Finally, we will talk about autocorrelation plot.\n","\n","Autocorrelation refers to the degree of correlation between the values of the same variables across different observations in the data.  The concept of autocorrelation is most often discussed in the context of [time series](https://www.statisticssolutions.com/resources/directory-of-statistical-analyses/time-series-analysis) data in which observations occur at different points in time (e.g., temperature measured on different days of the month, hours of the day).  For example, one might expect the air temperature on the $1^{st}$ day of the month to be more similar to the temperature on the $2^{nd}$ day compared to the $31^{st}$ day.  If the temperature values that occurred closer together in time are, in fact, more similar than the temperature values that occurred farther apart in time, the data would be autocorrelated.\n","\n","Autocorrelation can cause problems in conventional analyses (such as ordinary least squares regression) that assume independence of observations.\n","\n","In a regression analysis, autocorrelation of the regression residuals can also occur if the model is incorrectly specified.  For example, if you are attempting to model a simple linear relationship but the observed relationship is non-linear (i.e., it follows a curved or U-shaped function), then the residuals will be autocorrelated."]},{"cell_type":"code","metadata":{"id":"YNGVOfGsaDU2"},"source":["aq_df_na=aq_df.copy()\n","aq_df_na=aq_df_na.dropna()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2KqaJVMLRPqJ"},"source":["pd.plotting.autocorrelation_plot(aq_df_na['2014':'2016']['TEMP'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KpNMMdt_al-3"},"source":["Since our data is taken on a per hour basis, that's why the x-axis shows the number lagged hours.\n","\n","From the above plot, we can infer that after every year the data is repeating the trend. This is also true as the particular season will come always at a definite time of the year.\n","\n","**Note:** 8640 hours means one year. From this, we are able to infer the above visualization.\n","\n","But here, in the above plot, there are so many lags, which is very difficult to visualize. So, it is a discussion point that can we condense our data into monthly basis so that we can easily and conveniently visualize it. \n","\n","That's what we are going to do in next section using `resample` function in `pandas` library."]},{"cell_type":"code","metadata":{"id":"r7Yrd5CJZox5"},"source":["# sampling the data on monthly basis by taking mean for every month\n","aq_df_na['TEMP'].resample('1m').mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7fdfd2XddIRK"},"source":["pd.plotting.autocorrelation_plot(aq_df_na['2014':'2016']['TEMP'].resample('1m').mean())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OIHpFmGZeCBh"},"source":["Now, we can easily visualize here. At every 12 months (i.e. after every year) there is a repetition of peak meaning that the trend in temperature is repeating. \n","\n","Here, the two line shows the confidence interval (CI). The solid line indicates 90% CI and dash line represents 95% CI. The confidence interval shows the strength of autocorrelation in the data. The variation towards these CI lines represents more strength of autocorrelation."]},{"cell_type":"code","metadata":{"id":"yQV_As-tdxPB"},"source":["pd.plotting.autocorrelation_plot(aq_df_na['2014':'2016']['PM2.5'].resample('1m').mean())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3p2Gd2tagvgb"},"source":["Here, the data is within these lines, but that does not mean that there is no autocorrelation here. Maybe the strength of autocorrelation is less compared to that of temperature.\n","\n","Moreover, we can observe trends after every year (i.e. after every 12 months)."]},{"cell_type":"markdown","metadata":{"id":"kaBeJ0X3hscD"},"source":["With all these autocorrelation we are trying to infer that how our time series data will be better represented, what is the structure of it and what is pattern of your data like the trend, seasonality, etc."]}]}